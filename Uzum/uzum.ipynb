{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "\n",
    "reviews = pd.read_parquet(r\"C:\\Users\\muham\\Downloads\\reviews.parquet\")\n",
    "returns = pd.read_parquet(r\"C:\\Users\\muham\\Downloads\\returns.parquet\")\n",
    "test = pd.read_parquet(r\"C:\\Users\\muham\\Downloads\\test.parquet\")\n",
    "\n",
    "filey = open(r\"C:\\Users\\muham\\Desktop\\translated.txt\", 'w', encoding = 'utf-8')\n",
    "reviews = reviews[reviews['review_text'] != '']\n",
    "reviews = reviews.dropna(subset = 'review_text')\n",
    "# reviews['review_text'] = reviews['review_text'].where(reviews['review_text'] != '', 'neutral')\n",
    "\n",
    "mask = test['product_id'].isin(returns.drop_duplicates('product_id')[:200]['product_id'])\n",
    "must = test[~mask][:20]\n",
    "\n",
    "print(must)\n",
    "print(len(must))\n",
    "\n",
    "# mask = reviews['product_id'].isin(returns.drop_duplicates('product_id')[:200]['product_id'])\n",
    "# rr_pr = reviews[mask]\n",
    "\n",
    "# needed = rr_pr[:10]\n",
    "def trans(text):\n",
    "    result = GoogleTranslator(source='auto', target='english').translate(text)\n",
    "    return result\n",
    "\n",
    "# # translated = list(rr_pr['translation'])\n",
    "# needed['review_text'] = needed['review_text'].apply(trans)\n",
    "# csv_file_path = r\"C:\\Users\\muham\\Desktop\\_1.csv\"  # Desired file name\n",
    "# needed.to_csv(csv_file_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    " \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    " \n",
    "from nltk.corpus import stopwords \n",
    " \n",
    "from nltk.tokenize import word_tokenize \n",
    " \n",
    "from nltk.stem import WordNetLemmatizer \n",
    " \n",
    "import nltk \n",
    " \n",
    "nltk.download('all') \n",
    " \n",
    " \n",
    " \n",
    "def preprocess_text(text): \n",
    " \n",
    "    tokens = word_tokenize(text.lower()) \n",
    " \n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')] \n",
    " \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens] \n",
    " \n",
    "    processed_text = ' '.join(lemmatized_tokens) \n",
    " \n",
    "    return processed_text \n",
    " \n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text) \n",
    " \n",
    " \n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    " \n",
    "def get_sentiment(text): \n",
    " \n",
    "    scores = analyzer.polarity_scores(text) \n",
    " \n",
    "    sentiment = 1 if scores['pos'] > 0 else 0 \n",
    " \n",
    "    return sentiment \n",
    " \n",
    "df['sentiment'] = df['reviewText'].apply(get_sentiment)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code provided tries to solve the problem given by uzum for intership. it takes tabular data. tries to setimentilize given reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_absolute_error \n",
    " \n",
    " \n",
    "reasons = pd.get_dummies(reasons) \n",
    "model_1 = RandomForestRegressor(random_state=1) \n",
    "model_2 = RandomForestRegressor(random_state=1) \n",
    "model_3 = RandomForestRegressor(random_state=1) \n",
    "model_4 = RandomForestRegressor(random_state=1) \n",
    "model_5 = RandomForestRegressor(random_state=1) \n",
    " \n",
    " \n",
    "model_1.fit(train_X, cause_1) \n",
    "model_2.fit(train_X, cause_2) \n",
    "model_3.fit(train_X, cause_3) \n",
    "model_4.fit(train_X, cause_4) \n",
    "model_5.fit(train_X, cause_5) \n",
    " \n",
    "predictions = model_1.predict(val_X) \n",
    " \n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
